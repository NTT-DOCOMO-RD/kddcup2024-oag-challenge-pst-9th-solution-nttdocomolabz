{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install transformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install cogdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from cogdl.oag import oagbert\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 2024\n",
    "fix_seed(SEED)\n",
    "\n",
    "path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_context = pd.read_csv(os.path.join(path, 'df_train_context_filled_keywords.csv'))\n",
    "df_train_context = pd.read_csv(os.path.join(path, 'train_context_filled_citation.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list = []\n",
    "emb2_list = []\n",
    "cossim_list = []\n",
    "\n",
    "tokenizer, model = oagbert(\"oagbert-v2-sim\")\n",
    "# model.bert.to(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in df_train_context.iterrows():\n",
    "    if i%10==0:\n",
    "        print(i, time.time() - start_time)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # encode a paper\n",
    "    (\n",
    "        input_ids,\n",
    "        input_masks,\n",
    "        token_type_ids,\n",
    "        masked_lm_labels,\n",
    "        position_ids,\n",
    "        position_ids_second,\n",
    "        masked_positions,\n",
    "        num_spans,\n",
    "    ) = model.build_inputs(\n",
    "        title=str(row['title']), abstract=str(row['abstract']), venue=str(row['venue']), concepts=str(row['keywords'])\n",
    "    )\n",
    "    _, paper_embed_src = model.bert.forward(\n",
    "        input_ids=torch.LongTensor(input_ids).unsqueeze(0).to(device),\n",
    "        token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0).to(device),\n",
    "        attention_mask=torch.LongTensor(input_masks).unsqueeze(0).to(device),\n",
    "        output_all_encoded_layers=False,\n",
    "        checkpoint_activations=False,\n",
    "        position_ids=torch.LongTensor(position_ids).unsqueeze(0).to(device),\n",
    "        position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0).to(device),\n",
    "    )\n",
    "\n",
    "    (\n",
    "        input_ids,\n",
    "        input_masks,\n",
    "        token_type_ids,\n",
    "        masked_lm_labels,\n",
    "        position_ids,\n",
    "        position_ids_second,\n",
    "        masked_positions,\n",
    "        num_spans,\n",
    "    ) = model.build_inputs(\n",
    "        title=str(row['ref_title']), abstract=str(row['ref_abstract']), venue=str(row['ref_venue']), concepts=str(row['ref_keywords'])\n",
    "    )\n",
    "    _, paper_embed_src2 = model.bert.forward(\n",
    "        input_ids=torch.LongTensor(input_ids).unsqueeze(0).to(device),\n",
    "        token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0).to(device),\n",
    "        attention_mask=torch.LongTensor(input_masks).unsqueeze(0).to(device),\n",
    "        output_all_encoded_layers=False,\n",
    "        checkpoint_activations=False,\n",
    "        position_ids=torch.LongTensor(position_ids).unsqueeze(0).to(device),\n",
    "        position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0).to(device),\n",
    "    )\n",
    "\n",
    "    temp_cos_sim = util.cos_sim(paper_embed_src, paper_embed_src2).detach().cpu().numpy()[0][0]\n",
    "    emb_list.append(paper_embed_src.detach().cpu().numpy())\n",
    "    emb2_list.append(paper_embed_src2.detach().cpu().numpy())\n",
    "    cossim_list.append(temp_cos_sim)\n",
    "\n",
    "# np.save('train_context_emb1.npy', np.array(emb_list).reshape(-1,768))\n",
    "# np.save('train_context_emb2.npy', np.array(emb2_list).reshape(-1,768))\n",
    "np.save(f'{path}/train_context_cossim.npy', np.array(cossim_list).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pub_gen_context = pd.read_csv(os.path.join(path, 'test_pub_gen_context_filled_citation.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list = []\n",
    "emb2_list = []\n",
    "cossim_list = []\n",
    "\n",
    "tokenizer, model = oagbert(\"oagbert-v2-sim\")\n",
    "# model.bert.to(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in df_test_pub_gen_context.iterrows():\n",
    "    if i%10==0:\n",
    "        print(i, time.time() - start_time)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # encode a paper\n",
    "    (\n",
    "        input_ids,\n",
    "        input_masks,\n",
    "        token_type_ids,\n",
    "        masked_lm_labels,\n",
    "        position_ids,\n",
    "        position_ids_second,\n",
    "        masked_positions,\n",
    "        num_spans,\n",
    "    ) = model.build_inputs(\n",
    "        title=str(row['title']), abstract=str(row['abstract']), venue=str(row['venue']), concepts=str(row['keywords'])\n",
    "    )\n",
    "    _, paper_embed_src = model.bert.forward(\n",
    "        input_ids=torch.LongTensor(input_ids).unsqueeze(0).to(device),\n",
    "        token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0).to(device),\n",
    "        attention_mask=torch.LongTensor(input_masks).unsqueeze(0).to(device),\n",
    "        output_all_encoded_layers=False,\n",
    "        checkpoint_activations=False,\n",
    "        position_ids=torch.LongTensor(position_ids).unsqueeze(0).to(device),\n",
    "        position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0).to(device),\n",
    "    )\n",
    "\n",
    "    (\n",
    "        input_ids,\n",
    "        input_masks,\n",
    "        token_type_ids,\n",
    "        masked_lm_labels,\n",
    "        position_ids,\n",
    "        position_ids_second,\n",
    "        masked_positions,\n",
    "        num_spans,\n",
    "    ) = model.build_inputs(\n",
    "        title=str(row['ref_title']), abstract=str(row['ref_abstract']), venue=str(row['ref_venue']), concepts=str(row['ref_keywords'])\n",
    "    )\n",
    "    _, paper_embed_src2 = model.bert.forward(\n",
    "        input_ids=torch.LongTensor(input_ids).unsqueeze(0).to(device),\n",
    "        token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0).to(device),\n",
    "        attention_mask=torch.LongTensor(input_masks).unsqueeze(0).to(device),\n",
    "        output_all_encoded_layers=False,\n",
    "        checkpoint_activations=False,\n",
    "        position_ids=torch.LongTensor(position_ids).unsqueeze(0).to(device),\n",
    "        position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0).to(device),\n",
    "    )\n",
    "\n",
    "    temp_cos_sim = util.cos_sim(paper_embed_src, paper_embed_src2).detach().cpu().numpy()[0][0]\n",
    "    emb_list.append(paper_embed_src.detach().cpu().numpy())\n",
    "    emb2_list.append(paper_embed_src2.detach().cpu().numpy())\n",
    "    cossim_list.append(temp_cos_sim)\n",
    "\n",
    "# np.save('test_pub_gen_context_emb1.npy', np.array(emb_list).reshape(-1,768))\n",
    "# np.save('test_pub_gen_context_emb2.npy', np.array(emb2_list).reshape(-1,768))\n",
    "np.save(f'{path}/test_pub_gen_context_cossim.npy', np.array(cossim_list).reshape(-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pst-v5vgsebt-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
